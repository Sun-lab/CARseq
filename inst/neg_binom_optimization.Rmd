---
title: "Negtive binomial regression optimzation"
author: "Little Paul, Chong Jin, Wei Sun"
date: "10/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We seek to compare optimzation methods to obtain the MLE of a negative binomial regression with an interecept (X1) and 3 covariates: X2, X3, and X4. We consider the folloiwng four different optimzation methods that are defined through four functions in the following R codes.  

* function ```opt_glmNB```, which usees ```glm.nb``` function from R package ```MASS```. 

* function ```opt_Ropt```, which uses R function ```optim``` where both log likelihood and gradient are calculated using customized c functions. 

* function ```opt_nloptr```, which usees ```nloptr``` function from R package ```nloptr```. ```nloptr`` is an R Interface to NLopt that is a free/open-source library for nonlinear optimization

* function ```opt_RcppBFGS```: our own implementation of optimization using Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm. 


```{r libraries}
library(Rcpp)
library(RcppArmadillo)
library(MASS)
library(numDeriv)
library(matrixcalc)
library(nloptr)
```

```{r utility function}
smart_df = function(...){
	data.frame(...,stringsAsFactors=FALSE)
}

smart_append = function(new_fn, vec_fn){
	aa = file.create(new_fn)
	bb = sapply(vec_fn,function(x) file.append(new_fn,x))
}

init_pois = function(rr, all_dat){
	pois_out = glm(counts ~ X2 + X3 + X4 + offset(log_offset),
		data = all_dat[[rr]],family = "poisson")
	as.numeric(pois_out$coefficients)
}


opt_glmNB = function(rr, all_dat, init_beta){
  # glm.nb converge when |dev - dev_{old}|/(|dev| + 0.1) < epsilon.
	START = Sys.time()
	glm_nb_out = glm.nb(counts ~ X2 + X3 + X4 + offset(log_offset), 
	                    data = all_dat[[rr]], start = init_beta, init.theta = 1, 
	                    control = list(epsilon = 1e-7, maxit = 4e3, 
	                                   trace = FALSE))
	END = Sys.time()
	PAR = c(as.numeric(summary(glm_nb_out)$coefficients[,1]), 1/glm_nb_out$theta)
	list(PAR = PAR, START = START, END = END, 
	     TIME = as.numeric(difftime(END, START, units="secs")))
}

opt_Ropt = function(rr, all_dat, init_beta){
	vec_Y = all_dat[[rr]]$counts
	mat_X = as.matrix(all_dat[[rr]][, paste0("X",seq(4))])
	vec_O = all_dat[[rr]]$log_offset
	
	tmp_LL = function(PAR){
		Rcpp_NB_reg_LL(Y = vec_Y, X = mat_X, O = vec_O, PARAMS = PAR)
	}
	
	tmp_GRAD = function(PAR){
		as.numeric(Rcpp_NB_reg_GRAD(Y = vec_Y, X = mat_X ,O = vec_O, PARAMS = PAR))
	}
	
	START = Sys.time()
	init_PAR = c(init_beta,0)
	opt_out = optim(par = init_PAR, fn = tmp_LL, gr = tmp_GRAD, method = "BFGS",
		control = list(fnscale = -1,maxit = 4e3,abstol = 1e-7))
	
	END = Sys.time()
	list(PAR = c(opt_out$par[1:4], exp(opt_out$par[5])),
		START = START, END = END, 
		TIME = as.numeric(difftime(END,START,units="secs")))
}

opt_nloptr = function(rr,all_dat,init_beta){
  vec_Y = all_dat[[rr]]$counts
  mat_X = as.matrix(all_dat[[rr]][, paste0("X",seq(4))])
  vec_O = all_dat[[rr]]$log_offset
  
  tmp_LL = function(PAR){
    - Rcpp_NB_reg_LL_logscale_PHI(Y = vec_Y, X = mat_X, O = vec_O, 
                                  PARAMS = PAR)
  }
  
  tmp_GRAD = function(PAR){
    - as.numeric(Rcpp_NB_reg_GRAD_logscale_PHI(Y = vec_Y, X = mat_X, 
                                               O = vec_O, PARAMS = PAR))
  }
  
  lo = c(rep(-Inf, length(init_beta)), 0)
  hi = rep(Inf, length(init_beta) + 1)
  
  START    = Sys.time()
  init_PAR = c(init_beta,1)
  
  opts = nloptr::nl.opts()
  opts["algorithm"] = "NLOPT_LD_LBFGS"
  opts["maxeval"]   = 4e3
  opts["ftol_abs"]  = 1e-7
  opts["xtol_abs"]  = 1e-7
  opts["xtol_rel"]  = 0
  
  opt_out = nloptr::nloptr(x0 = init_PAR, eval_f = tmp_LL, 
                           eval_grad_f = tmp_GRAD, opts = opts,
                           lb = lo, ub = hi)
  
  END = Sys.time()
  
  list(PAR = c(opt_out$solution[1:4],opt_out$solution[5]), 
       START = START, END = END, 
       TIME = as.numeric(difftime(END,START,units="secs")))
}

opt_RcppBFGS = function(rr,all_dat,init_beta){
	vec_Y = all_dat[[rr]]$counts
	mat_X = as.matrix(all_dat[[rr]][,paste0("X",seq(4))])
	vec_O = all_dat[[rr]]$log_offset
	
	START = Sys.time()
	
	init_PAR = c(init_beta,0)
	rcpp_out = Rcpp_NB_reg_BFGS(Y = vec_Y, X = mat_X, O = vec_O, 
		params0 = init_PAR, max_iter = 4e3, eps = 1e-7, show = FALSE)
	
	END = Sys.time()
	
	list(PAR = c(rcpp_out$PAR[1:4],exp(rcpp_out$PAR[5])), 
		START = START, END = END, 
		TIME = as.numeric(difftime(END,START,units="secs")))
}

sourceCpp("test_NBreg.cpp", showOutput=FALSE)

```

Next we simulated count data from a negative binomial distribution. We simuatled 1000 datasets. Each dataset has 100 samples and four covariates, including intercept (X1). 
```{r}
set.seed(100)

true_beta = c(3, 0.25, -0.5, 1)
true_phi  = 0.2
all_dat   = list()
num_reps  = 1000
maxN      = 100

for(rr in seq(num_reps)){
  
  tmp_dat = smart_df(X1 = 1, X2 = runif(maxN,-1,1),
                     X3 = rbinom(maxN,1,0.5), 
                     X4 = rnorm(maxN,mean = 3,sd = 0.1))
  
  tmp_dat$log_offset = log(runif(maxN,0,4))
  
  X_matrix = as.matrix(tmp_dat[,paste0("X",seq(4))])
  vec_log_mean = X_matrix %*% true_beta + tmp_dat$log_offset
  
  tmp_dat$counts = rnbinom(maxN, mu = exp(vec_log_mean), size = 1/true_phi)
  
  all_dat[[rr]] = tmp_dat
}
```

Then we perform four types of optimizations. 

```{r}
all_res = c()

for(rr in seq(num_reps)){
  
	if(rr %% 50 == 0) cat(paste0(rr, " "))
	if(rr %% 500 == 0) cat("\n")

  init_beta  = init_pois(rr = rr, all_dat = all_dat)
  
  Ropt       = opt_Ropt(rr = rr, all_dat = all_dat, init_beta = init_beta)
  glmNB      = opt_glmNB(rr = rr, all_dat = all_dat, init_beta = init_beta)
  nloptr_dat = opt_nloptr(rr = rr, all_dat = all_dat, init_beta = init_beta)
  RcppBFGS   = opt_RcppBFGS(rr = rr, all_dat = all_dat, init_beta = init_beta)
  
  varnms = c(paste0("BETA",seq(4)), "PHI")
  
  tmp_dat1   = smart_df(rr = rr, METHOD = "glmNB", VAR = varnms, 
                        PAR = glmNB$PAR, TRUTH = c(true_beta, true_phi), 
                        TIME = glmNB$TIME)
  
  tmp_dat2   = smart_df(rr = rr, METHOD = "nloptr", VAR = varnms, 
                        PAR = nloptr_dat$PAR, TRUTH = c(true_beta, true_phi),
                        TIME = nloptr_dat$TIME)
  
  tmp_dat3 = smart_df(rr = rr, METHOD = "Ropt", VAR = varnms, 
                      PAR = Ropt$PAR, TRUTH = c(true_beta, true_phi), 
                      TIME = Ropt$TIME)
  
  tmp_dat4 = smart_df(rr = rr, METHOD = "RcppBFGS", VAR = varnms, 
                      PAR = RcppBFGS$PAR, TRUTH = c(true_beta, true_phi), 
                      TIME = RcppBFGS$TIME)
  
  all_res = rbind(all_res,tmp_dat1,tmp_dat2,tmp_dat3, tmp_dat4)
}

all_res[1:6,]
```


The following scatter plots compare the estimtes of all the parameters across the four optimzation methods. It is clear that Ropt fails in a substantional fraction of cases, and all the other three methods reach the same results across all replicates. The failure of Ropt is probably due to the transformation of gradient in log scale and the gradient is relatively flat.

```{r fig.width=7.5, fig.height=12.5, fig.path='figures/', dev=c('png', 'pdf')}

all_vars = c(paste0("BETA",seq(4)), "PHI")

par(mfrow=c(5,3), mar=c(5,4,4,1), bty="n")

for(tmp_var in all_vars){
  # tmp_var = all_vars[1]
  sub_res = all_res[which(all_res$VAR == tmp_var),]
  
  tmp_dat = smart_df(glmNB = sub_res$PAR[which(sub_res$METHOD == "glmNB")],
                     Ropt  = sub_res$PAR[which(sub_res$METHOD == "Ropt")],
                     RcppBFGS = sub_res$PAR[which(sub_res$METHOD == "RcppBFGS")],
                     nloptr   = sub_res$PAR[which(sub_res$METHOD == "nloptr")])
  
  plot(tmp_dat[,c("glmNB", "Ropt")], pch=16, col=rgb(0,0,0,0.5), main=tmp_var)
  abline(a=0, b=1, lty=2)
  
  plot(tmp_dat[,c("glmNB","nloptr")],pch=16,col=rgb(0,0,0,0.5),main=tmp_var)
  abline(a=0,b=1,lty=2)
  
  plot(tmp_dat[,c("glmNB", "RcppBFGS")],pch=16,col=rgb(0,0,0,0.5),main=tmp_var)
  abline(a=0,b=1,lty=2)
}
```

We further check the distribution of the parameter estimates. 

```{r fig.width=12, fig.height=8, fig.path='figures/', dev=c('png', 'pdf')}

par(mfrow=c(4,5), mar=c(5,4,2,1), bty="n")
tmp_range = range(all_res$TIME)

for(mm in c("glmNB", "Ropt", "RcppBFGS", "nloptr")){
  count = 1
  
  for(vv in c(paste0("BETA", seq(4)), "PHI")){
    hist(all_res$PAR[which(all_res$VAR == vv & all_res$METHOD == mm)],
         main="",xlab=vv, freq=FALSE, col="gray", breaks=30)
    
    abline(v=c(true_beta, true_phi)[count], col="blue", lwd=3, lty=1)
    ww1 = which(all_res$VAR == vv & all_res$METHOD == mm)
    abline(v=mean(all_res$PAR[ww1]), col="red", lwd=3, lty=2)
    abline(v=median(all_res$PAR[ww1]), col="green", lwd=3, lty=3)
    
    if(mm == "Ropt" && count == 1){
      legend("topright", legend=c("truth", "mean", "median"), 
             col=c("blue","red","green"), lwd=rep(3,3), lty=seq(3))
    }
    
    if(count == 1){
      title(sub=mm)
    }
    
    count = count + 1
  }
  
}

```

Finally we compare computational time. While ```Ropt``` is the fastest one, it failes in a substantial number of cases. In the remaining methods, the computationa time is RcppBFGS < nloptr < glmNB. Although nloptr is also written in C++, it is still slower than our BFGS implmentation, probably because nloptr is a more heavyweight package that can support multiple optimization algorithms and numerous stopping criteria. 

Note that we have used simlar stopping criterion for the methods to compare with:

glmNB: epsilon = 1e-7. Here epsilon is used to compare in this fashion: "|dev - dev_{old}|/(|dev| + 0.1) < epsilon", which is a combination of the relative and absolute difference in function value f.

RcppBFGS: eps = 1e-7, and we have used both norm(x_previous - x_current) < eps and abs(f_previous - f_current) < eps, where x is the parameter to optimize while f is the function value.

nloptr (L-BFGS): ftol_abs = 1e-7, xtol_abs = 1e-7. Then the stopping criteria will be (max of element-wise abs) (x_previous - x_current) < eps and abs(f_previous - f_current) < eps. 


```{r computation_time, fig.width=3.5, fig.height=3.5, fig.path='figures/', dev=c('png', 'pdf')}
tapply(all_res$TIME, all_res$METHOD, summary)

par(mfrow=c(1,1), mar=c(5,4,4,1), bty="n")

plot(c(-3.5, 0), c(0, 10.5), type="n", xlab="log10(Time, seconds)", 
     ylab="density")

cols = c("orange", "red", "blue")
ltys = c(1, 2, 4)
methods = c("glmNB", "nloptr", "RcppBFGS")

for(k in 1:length(methods)){
  time.v = log10(all_res$TIME[which(all_res$METHOD == methods[k])])
  lines(density(time.v), col=cols[k], lty=ltys[k], lwd=1.5)
}

legend("topright", bty="n", legend=methods, col=cols, lty=ltys, 
       lwd=rep(1.5,3))
```


```{r}
sessionInfo()
```


